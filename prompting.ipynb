{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d723084-45d9-49ad-afd4-90f914c2a555",
   "metadata": {},
   "source": [
    "# Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5583d3e7-0a2a-4236-8b39-5bf863f171fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from IPython.display import HTML, Markdown, display\n",
    "import os\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27611df5-52e0-4c6b-980f-29e82e8d494a",
   "metadata": {},
   "source": [
    "Single-turn, text-in/text-out  structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd2ae14a-afa6-4f8f-8955-485e78fee8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine you have a really smart robot friend. This robot can learn things just like you do! You can teach it by showing it pictures, telling it stories, or playing games with it. The more you teach it, the smarter it gets.\n",
      "\n",
      "That's kind of like AI (Artificial Intelligence). It's a computer program that can learn and solve problems like a human. AI can do lots of cool things, like:\n",
      "\n",
      "* **Play games:** AI can beat you at chess or even video games!\n",
      "* **Answer questions:** Ask AI a question and it can try to give you an answer.\n",
      "* **Recognize faces:** AI can look at pictures and tell you who's in them.\n",
      "* **Write stories:** Some AIs can even write stories and poems.\n",
      "\n",
      "AI is still learning and getting smarter every day. It can help us do many things, like make our lives easier, learn new things, and even solve problems in the world. It's like having a really cool robot friend that can help us do amazing things! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "flash = genai.GenerativeModel('gemini-1.5-flash')\n",
    "response = flash.generate_content(\"Explain AI to me like I'm a kid.\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "228c9cd9-821f-4a80-baca-936697b92bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Imagine you have a really smart robot friend. This robot can learn things just like you do! You can teach it by showing it pictures, telling it stories, or playing games with it. The more you teach it, the smarter it gets.\n",
       "\n",
       "That's kind of like AI (Artificial Intelligence). It's a computer program that can learn and solve problems like a human. AI can do lots of cool things, like:\n",
       "\n",
       "* **Play games:** AI can beat you at chess or even video games!\n",
       "* **Answer questions:** Ask AI a question and it can try to give you an answer.\n",
       "* **Recognize faces:** AI can look at pictures and tell you who's in them.\n",
       "* **Write stories:** Some AIs can even write stories and poems.\n",
       "\n",
       "AI is still learning and getting smarter every day. It can help us do many things, like make our lives easier, learn new things, and even solve problems in the world. It's like having a really cool robot friend that can help us do amazing things! \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab42552-e097-48b9-947f-8ded6a351567",
   "metadata": {},
   "source": [
    "**Start a chat**\n",
    "\n",
    "multi-turn chat structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7d76218-5923-437e-9444-427d1897650c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Maharajan! It's nice to meet you. ðŸ˜Š  What can I do for you today? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat  = flash.start_chat(history=[])\n",
    "response = chat.send_message(\"Hello! My name is Maharajan.\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b56a0ac-5f2a-43cd-8d38-93eaaf344d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You bet! Dinosaurs are fascinating creatures, and there's a lot to learn about them. Here are a few interesting facts:\n",
      "\n",
      "* **The biggest land animal ever was a dinosaur:** The Argentinosaurus, a long-necked sauropod, could have reached up to 100 feet long and weighed over 100 tons! That's bigger than a Boeing 737 airplane!\n",
      "\n",
      "* **Some dinosaurs had feathers:**  While we often imagine dinosaurs as scaly beasts, many, including the Velociraptor, had feathers.  Some even used them for display or flight!\n",
      "\n",
      "* **Dinosaurs ruled the Earth for over 180 million years:** That's a very long time! During that time, they evolved into a vast array of shapes, sizes, and lifestyles. \n",
      "\n",
      "* **Dinosaurs lived on all the continents:** Even Antarctica, which was much warmer millions of years ago, was home to dinosaurs!\n",
      "\n",
      "* **Not all dinosaurs were giants:**  While some were truly enormous, many were smaller than a chicken.  The Compsognathus, one of the smallest, was about the size of a turkey. \n",
      "\n",
      "* **The T-Rex wasn't the only terrifying dinosaur:**  There were many other fearsome predators like the Giganotosaurus, the Carnotaurus, and the Spinosaurus.\n",
      "\n",
      "* **We're still discovering new dinosaur species:**  Paleontologists are constantly unearthing new dinosaur fossils, expanding our knowledge of these incredible creatures.\n",
      "\n",
      "Would you like to learn more about a particular type of dinosaur?  Perhaps you'd like to hear a story about how a dinosaur fossil was discovered? Just let me know! ðŸ˜Š \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Can you tell something interesting about dinosaurs\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7405e2e-1f7b-48f4-9947-0c30720a271e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course! You are Maharajan.  ðŸ˜Š  It's great to be talking with you.  Do you want to continue learning about dinosaurs, or is there something else you'd like to chat about? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# While you have the `chat` object around, the conversation state\n",
    "# persists. Confirm that by asking if it knows my name.\n",
    "response = chat.send_message('Do you remember what my name is?')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414eaad6-f1bc-4d41-a70c-ce8fe1c7e66f",
   "metadata": {},
   "source": [
    "**Choose a model**\n",
    "\n",
    "Available model and their capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce0348c7-3376-4e97-8f7d-77c8b71a45bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro\n",
      "models/gemini-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-pro-exp-0801\n",
      "models/gemini-1.5-pro-exp-0827\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-exp-0827\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/aqa\n"
     ]
    }
   ],
   "source": [
    "for model in genai.list_models():\n",
    "    print(model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f541eab-76ce-4074-b59f-ec87808e27e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(name='models/gemini-1.5-flash',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash',\n",
      "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n"
     ]
    }
   ],
   "source": [
    "for model in genai.list_models():\n",
    "    if model.name == 'models/gemini-1.5-flash':\n",
    "        print(model)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46da24a9-21e8-4383-ad07-95b074ad2b3a",
   "metadata": {},
   "source": [
    "## explore generation parameters\n",
    "\n",
    "**Output length**\n",
    "\n",
    "When generating text with an LLM, the output length affects cost and performance. Generating more tokens increases computation, leading to higher energy consumption, latency, and cost.\n",
    "\n",
    "To stop the model from generating tokens past a limit, you can specify the `max_output_tokens` parameter when using the API.\n",
    "\n",
    "Prompt engineering may be required to generate a more complete output for your given limit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce0d191c-eb0d-49f5-87d9-de2459508bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## The Enduring Significance of Olives: A Modern Tale of Antiquity\n",
      "\n",
      "The olive tree, a symbol of peace, longevity, and prosperity, has graced the Mediterranean landscape for millennia. Its fruit, the olive, has transcended its culinary role to become a cornerstone of various aspects of modern society, weaving its influence through food, health, and even the very fabric of the environment. This essay will explore the diverse and enduring significance of olives in the 21st century, highlighting their multifaceted impact on our lives.\n",
      "\n",
      "Firstly, olives are undeniably a culinary staple in many cultures. The distinct, briny flavor and versatility of olives have made them a ubiquitous ingredient in Mediterranean cuisine. From the classic Greek olive oil and Kalamata olives to the vibrant tapenade spread in France, olives add a unique depth of flavor to countless dishes. Their versatility extends beyond savory applications as well. Olives are used in salads, pizzas, pasta sauces, and even desserts, showcasing their potential to transform both traditional and modern\n"
     ]
    }
   ],
   "source": [
    "short_model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config=genai.GenerationConfig(max_output_tokens=200))\n",
    "\n",
    "response = short_model.generate_content('Write a 1000 word essay on the importance of olives in modern society.')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f6cef80-c682-41f1-8c84-f95a7b5791f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A tiny fruit, a verdant sphere,\n",
      "A symbol of the earth held dear.\n",
      "The olive, small, yet mighty strong,\n",
      "A taste of history, all along.\n",
      "\n",
      "From ancient feasts to modern plates,\n",
      "It graces tables, conquers fates.\n",
      "In oil it sings, a golden gleam,\n",
      "A culinary masterpiece, it would seem.\n",
      "\n",
      "From salads bright to savory bread,\n",
      "Its flavor dances, unsaid.\n",
      "A source of health, a vibrant hue,\n",
      "The olive's presence, ever true.\n",
      "\n",
      "So raise a glass, a toast we share,\n",
      "To this small fruit, beyond compare.\n",
      "For in its essence, we find grace,\n",
      "The olive's story, in time and space. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = short_model.generate_content('Write a short poem on the importance of olives in modern society.')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0ee33d-2682-4352-961f-d2a9db4bf1c1",
   "metadata": {},
   "source": [
    "**Temprature**\n",
    "\n",
    "Temperature controls the degree of randomness in token selection. Higher temperatures result in a higher number of candidate tokens from which the next output token is selected, and can produce more diverse results, while lower temperatures have the opposite effect, such that a temperature of 0 results in greedy decoding, selecting the most probable token at each step.\n",
    "\n",
    "Temperature doesn't provide any guarantees of randomness, but it can be used to \"nudge\" the output somewhat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f856f20-ccc1-4f0b-96fd-79702e9aeea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teal \n",
      " -------------------------\n",
      "Purple \n",
      " -------------------------\n",
      "Blue. \n",
      " -------------------------\n",
      "Purple \n",
      " -------------------------\n",
      "Purple. \n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "from google.api_core import retry\n",
    "\n",
    "high_temp_model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config=genai.GenerationConfig(temperature=2.0))\n",
    "\n",
    "\n",
    "# When running lots of queries, it's a good practice to use a retry policy so your code\n",
    "# automatically retries when hitting Resource Exhausted (quota limit) errors.\n",
    "retry_policy = {\n",
    "    \"retry\": retry.Retry(predicate=retry.if_transient_error, initial=10, multiplier=1.5, timeout=300)\n",
    "}\n",
    "\n",
    "for _ in range(5):\n",
    "  response = high_temp_model.generate_content('Pick a random colour... (respond in a single word)',\n",
    "                                              request_options=retry_policy)\n",
    "  if response.parts:\n",
    "    print(response.text, '-' * 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7cd142-085a-456a-9819-244c19886497",
   "metadata": {},
   "source": [
    "Now try the same prompt with temperature set to zero. Note that the output is not completely deterministic, as other parameters affect token selection, but the results will tend to be more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d6fa6fb-23cc-43b3-ba29-ef6ea631ba82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purple \n",
      " -------------------------\n",
      "Purple \n",
      " -------------------------\n",
      "Purple \n",
      " -------------------------\n",
      "Purple \n",
      " -------------------------\n",
      "Purple \n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "low_temp_model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config=genai.GenerationConfig(temperature=0.0))\n",
    "\n",
    "for _ in range(5):\n",
    "  response = low_temp_model.generate_content('Pick a random colour... (respond in a single word)',\n",
    "                                             request_options=retry_policy)\n",
    "  if response.parts:\n",
    "    print(response.text, '-' * 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20abfc05-1ce3-4f1a-821a-7f16db91b681",
   "metadata": {},
   "source": [
    "**Top-K and top-P**\n",
    "\n",
    "Like temperature, top-K and top-P parameters are also used to control the diversity of the model's output.\n",
    "\n",
    "Top-K is a positive integer that defines the number of most probable tokens from which to select the output token. A top-K of 1 selects a single token, performing greedy decoding.\n",
    "\n",
    "Top-P defines the probability threshold that, once cumulatively exceeded, tokens stop being selected as candidates. A top-P of 0 is typically equivalent to greedy decoding, and a top-P of 1 typically selects every token in the model's vocabulary.\n",
    "\n",
    "When both are supplied, the Gemini API will filter top-K tokens first, then top-P and then finally sample from the candidate tokens using the supplied temperature.\n",
    "\n",
    "Run this example a number of times, change the settings and observe the change in output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df8db1b8-6acd-4131-9e5f-084cdbb9122a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bartholomew, a ginger tabby with a heart of gold and a belly perpetually rumbling with hunger, was tired of the routine. The same old food bowl, the same old window perch, the same old scratching post â€“ it all felt a little tooâ€¦ predictable. One day, as the sun dipped below the horizon, casting long shadows across the living room, a daring idea sparked in his emerald eyes. He would go on an adventure. \n",
      "\n",
      "He snuck out through the cat flap, the cool night air tingling his whiskers. The world was a symphony of scents â€“ damp earth, honeysuckle, and the intoxicating aroma of a distant bakery. Bartholomew, his tail held high, set off towards the beckoning darkness. \n",
      "\n",
      "He navigated the maze of back alleys, dodging stray dogs and avoiding the grumpy old lady who always threw water at him. His journey led him to the heart of the city, where towering buildings scraped the sky and neon signs flickered like fireflies. He climbed a fire escape, his claws finding purchase on the rusty metal, and gazed upon the city lights, a sparkling sea of human activity. \n",
      "\n",
      "He then met a streetwise alley cat named Whiskers. Whiskers, with his tattered ears and one eye that looked permanently surprised, was a connoisseur of the city's hidden treasures. He led Bartholomew to a secret garden, a paradise hidden in the concrete jungle, where flowers bloomed in vibrant hues and a gurgling fountain provided a calming melody. There, Bartholomew learned to decipher the language of birds, tasted the sweetness of forbidden fruit, and even discovered a love for the rhythmic sway of the moonlit grass.\n",
      "\n",
      "The night, however, was not without its dangers. A monstrous creature, with flashing headlights and a deafening growl, threatened their peace. It was a truck, and it was too big, too loud, too scary.  Whiskers, however, had a plan.  He led Bartholomew through a narrow tunnel, a secret escape route known only to the most experienced alley cats. \n",
      "\n",
      "Dawn painted the sky with a palette of pink and orange as they emerged from the tunnel, back into the familiar streets.  Bartholomew, his heart full of the thrill of adventure, knew he could never truly be the same again. He had seen the city's beauty, its darkness, and its hidden secrets. \n",
      "\n",
      "Returning home, he found his food bowl empty. The humans had left for work, but he was no longer hungry. He had found something far more nourishing than tuna â€“ he had found adventure, a sense of freedom, and the joy of being a cat who dared to explore beyond the walls of his comfortable routine.  \n",
      "\n",
      "As he curled up on the sunny windowsill, he knew that this was not the end of his adventures. It was just the beginning. He would find a new secret every day, a new scent to follow, a new story to write.  And he would never again be just Bartholomew, the cat. He would be Bartholomew, the explorer, the adventurer, the cat who lived a thousand lives in the heart of the city. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-001',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        # These are the default values for gemini-1.5-flash-001.\n",
    "        temperature=1.0,\n",
    "        top_k=64,\n",
    "        top_p=0.95,\n",
    "    ))\n",
    "\n",
    "story_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\n",
    "response = model.generate_content(story_prompt, request_options=retry_policy)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6e4561-12d6-4787-aa37-30044b8524e8",
   "metadata": {},
   "source": [
    "## Prompting\n",
    "\n",
    "**Zero-shot**\n",
    "\n",
    "Zero-shot prompts are prompts that describe the request for the model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4208a9c6-470a-4fa9-b42f-8c5d49a54de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: **POSITIVE**\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-001',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=5,\n",
    "    ))\n",
    "\n",
    "zero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
    "Review: \"Her\" is a disturbing study revealing the direction\n",
    "humanity is headed if AI is allowed to keep evolving,\n",
    "unchecked. I wish there were more movies like this masterpiece.\n",
    "Sentiment: \"\"\"\n",
    "\n",
    "response = model.generate_content(zero_shot_prompt, request_options=retry_policy)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2534aa0d-ce9a-4fb8-9165-4d7208b3f6d5",
   "metadata": {},
   "source": [
    "**Enum mode\n",
    "\n",
    "The models are trained to generate text, and can sometimes produce more text than you may wish for. In the preceding example, the model will output the label, sometimes it can include a preceding \"Sentiment\" label, and without an output token limit, it may also add explanatory text afterwards.\n",
    "\n",
    "The Gemini API has an Enum mode feature that allows you to constrain the output to a fixed set of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afe91a6d-ec52-42cc-9ca8-760bb802b0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "import enum\n",
    "\n",
    "class Sentiment(enum.Enum):\n",
    "    POSITIVE = \"positive\"\n",
    "    NEUTRAL = \"neutral\"\n",
    "    NEGATIVE = \"negative\"\n",
    "\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-001',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        response_mime_type=\"text/x.enum\",\n",
    "        response_schema=Sentiment\n",
    "    ))\n",
    "\n",
    "response = model.generate_content(zero_shot_prompt, request_options=retry_policy)\n",
    "print(response.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93045d7a-a75e-4c70-9034-ae4fdf8593c1",
   "metadata": {},
   "source": [
    "**One-shot and few-shot**\n",
    "\n",
    "Providing an example of the expected response is known as a \"one-shot\" prompt. When you provide multiple examples, it is a \"few-shot\" prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "956a5ad8-52ff-4293-9fd1-395be75686a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\"size\": \"large\",\n",
      "\"type\": \"normal\",\n",
      "\"ingredients\": [\"cheese\", \"pineapple\"]\n",
      "}\n",
      "``` \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-latest',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=250,\n",
    "    ))\n",
    "\n",
    "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
    "\n",
    "EXAMPLE:\n",
    "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"small\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"cheese\", \"tomato sauce\", \"peperoni\"]\n",
    "}\n",
    "```\n",
    "\n",
    "EXAMPLE:\n",
    "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"large\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
    "}\n",
    "\n",
    "ORDER:\n",
    "\"\"\"\n",
    "\n",
    "customer_order = \"Give me a large with cheese & pineapple\"\n",
    "\n",
    "\n",
    "response = model.generate_content([few_shot_prompt, customer_order], request_options=retry_policy)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddc96d7-7236-487e-9fd8-c581d5a1bb75",
   "metadata": {},
   "source": [
    "**JSON mode**\n",
    "\n",
    "To provide control over the schema, and to ensure that you only receive JSON (with no other text or markdown), you can use the Gemini API's JSON mode. This forces the model to constrain decoding, such that token selection is guided by the supplied schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5aa60bbe-cd77-434e-a284-a369b9f1479f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"ingredients\": [\"apple\", \"chocolate\"], \"size\": \"large\", \"type\": \"dessert\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import typing_extensions as typing\n",
    "\n",
    "class PizzaOrder(typing.TypedDict):\n",
    "    size: str\n",
    "    ingredients: list[str]\n",
    "    type: str\n",
    "\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-latest',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=PizzaOrder,\n",
    "    ))\n",
    "\n",
    "response = model.generate_content(\"Can I have a large dessert pizza with apple and chocolate\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2a8d4d-7777-4853-a35e-610f6d35611d",
   "metadata": {},
   "source": [
    "**Chain of Thought (CoT)**\n",
    "\n",
    "Direct prompting on LLMs can return answers quickly and (in terms of output token usage) efficiently, but they can be prone to hallucination. The answer may \"look\" correct (in terms of language and syntax) but is incorrect in terms of factuality and reasoning.\n",
    "\n",
    "Chain-of-Thought prompting is a technique where you instruct the model to output intermediate reasoning steps, and it typically gets better results, especially when combined with few-shot examples. It is worth noting that this technique doesn't completely eliminate hallucinations, and that it tends to cost more to run, due to the increased token count.\n",
    "\n",
    "As models like the Gemini family are trained to be \"chatty\" and provide reasoning steps, you can ask the model to be more direct in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd3da12e-36f3-498b-81c8-0e747c33c779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
    "am 20 years old. How old is my partner? Return the answer directly.\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "response = model.generate_content(prompt, request_options=retry_policy)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ab7f527-f096-48e1-9a7b-7b51873cf6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's how to solve this:\n",
      "\n",
      "* **When you were 4:** Your partner was 3 times your age, meaning they were 4 * 3 = 12 years old.\n",
      "* **Age difference:** Your partner is 12 - 4 = 8 years older than you.\n",
      "* **Current age:** Since you are now 20 years old, your partner is 20 + 8 = **28 years old**. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\n",
    "I am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n",
    "\n",
    "response = model.generate_content(prompt, request_options=retry_policy)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1142a3d9-c3cd-447f-85a0-1fc7181c8162",
   "metadata": {},
   "source": [
    "**ReAct: Reason and act**\n",
    "\n",
    "In this example you will run a ReAct prompt directly in the Gemini API and perform the searching steps yourself. As this prompt follows a well-defined structure, there are frameworks available that wrap the prompt into easier-to-use APIs that make tool calls automatically, such as the LangChain example from the chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45592bca-1392-48c1-901c-43c010df3d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instructions = \"\"\"\n",
    "Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
    "Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
    " (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
    "     will return some similar entities to search and you can try to search the information from those topics.\n",
    " (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
    "     so keep your searches short.\n",
    " (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
    "\"\"\"\n",
    "\n",
    "example1 = \"\"\"Question\n",
    "Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
    "\n",
    "Thought 1\n",
    "The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
    "\n",
    "Action 1\n",
    "<search>Milhouse</search>\n",
    "\n",
    "Observation 1\n",
    "Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
    "\n",
    "Thought 2\n",
    "The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
    "\n",
    "Action 2\n",
    "<lookup>named after</lookup>\n",
    "\n",
    "Observation 2\n",
    "Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
    "\n",
    "Thought 3\n",
    "Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
    "\n",
    "Action 3\n",
    "<finish>Richard Nixon</finish>\n",
    "\"\"\"\n",
    "\n",
    "example2 = \"\"\"Question\n",
    "What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
    "\n",
    "Thought 1\n",
    "I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
    "\n",
    "Action 1\n",
    "<search>Colorado orogeny</search>\n",
    "\n",
    "Observation 1\n",
    "The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
    "\n",
    "Thought 2\n",
    "It does not mention the eastern sector. So I need to look up eastern sector.\n",
    "\n",
    "Action 2\n",
    "<lookup>eastern sector</lookup>\n",
    "\n",
    "Observation 2\n",
    "The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
    "\n",
    "Thought 3\n",
    "The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
    "\n",
    "Action 3\n",
    "<search>High Plains</search>\n",
    "\n",
    "Observation 3\n",
    "High Plains refers to one of two distinct land regions\n",
    "\n",
    "Thought 4\n",
    "I need to instead search High Plains (United States).\n",
    "\n",
    "Action 4\n",
    "<search>High Plains (United States)</search>\n",
    "\n",
    "Observation 4\n",
    "The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n",
    "\n",
    "Thought 5\n",
    "High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
    "\n",
    "Action 5\n",
    "<finish>1,800 to 7,000 ft</finish>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b188a4-0a26-4e47-8398-a1141ffa65ec",
   "metadata": {},
   "source": [
    "To capture a single step at a time, while ignoring any hallucinated Observation steps, you will use `stop_sequences` to end the generation process. The steps are `Thought,` `Action,` `Observation,` in that order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17c25884-903c-4173-8e37-e32aa92f0688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 1\n",
      "I need to search for the transformers NLP paper and look for the authors list. Then, I need to find the youngest author.\n",
      "\n",
      "Action 1\n",
      "<search>transformers NLP paper</search>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Question\n",
    "Who was the youngest author listed on the transformers NLP paper?\n",
    "\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "react_chat = model.start_chat()\n",
    "\n",
    "# You will perform the Action, so generate up to, but not including, the Observation.\n",
    "config = genai.GenerationConfig(stop_sequences=[\"\\nObservation\"])\n",
    "\n",
    "resp = react_chat.send_message(\n",
    "    [model_instructions, example1, example2, question],\n",
    "    generation_config=config,\n",
    "    request_options=retry_policy)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0499d54d-4e5d-47e9-8466-b23be31db4f5",
   "metadata": {},
   "source": [
    "Now you can perform this research yourself and supply it back to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c16e0ab-c48f-45c8-b075-61ada2dd9185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 2\n",
      "The authors are listed in the first paragraph. I need to find the youngest author. I do not have enough information to determine the youngest author.\n",
      "\n",
      "Action 2\n",
      "<finish>I need more information to answer this question. I cannot determine the youngest author from the provided information.</finish> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "observation = \"\"\"Observation 1\n",
    "[1706.03762] Attention Is All You Need\n",
    "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
    "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
    "\"\"\"\n",
    "resp = react_chat.send_message(observation, generation_config=config, request_options=retry_policy)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fac91b5-d2e9-45f3-b51f-ed8ea9cb55d1",
   "metadata": {},
   "source": [
    "This process repeats until the <finish> action is reached. You can continue running this yourself if you like, or try the [Wikipedia example](https://github.com/google-gemini/cookbook/blob/main/examples/Search_Wikipedia_using_ReAct.ipynb) to see a fully automated ReAct system at work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabba2b6-c43c-4345-ab7f-ca095566d071",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
